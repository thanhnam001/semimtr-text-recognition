import torch
from torch import nn
from .vision_transformer import PatchEmbed, CBlock, Block
from einops import rearrange
import math
import warnings
from semimtr.utils.utils import if_none
import logging

def _trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):
        # Computes standard normal cumulative distribution function
        return (1. + math.erf(x / math.sqrt(2.))) / 2.

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
                      "The distribution of values may be incorrect.",
                      stacklevel=2)

    # Values are generated by using a truncated uniform distribution and
    # then using the inverse CDF for the normal distribution.
    # Get upper and lower cdf values
    l = norm_cdf((a - mean) / std)
    u = norm_cdf((b - mean) / std)

    # Uniformly fill tensor with values from [l, u], then translate to
    # [2l-1, 2u-1].
    tensor.uniform_(2 * l - 1, 2 * u - 1)

    # Use inverse cdf transform for normal distribution to get truncated
    # standard normal
    tensor.erfinv_()

    # Transform to proper mean, std
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # Clamp to ensure it's in the proper range
    tensor.clamp_(min=a, max=b)
    return tensor


def trunc_normal_(tensor, mean=0., std=1., a=-2., b=2.):
    # type: (Tensor, float, float, float, float) -> Tensor
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    NOTE: this impl is similar to the PyTorch trunc_normal_, the bounds [a, b] are
    applied while sampling the normal with mean/std applied, therefore a, b args
    should be adjusted to match the range of mean, std args.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value
    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    with torch.no_grad():
        return _trunc_normal_(tensor, mean, std, a, b)

class ConvViT(nn.Module):
    """ 
    VisionTransformer backbone from MCMAE for downstream task
    """
    def __init__(self,config):
        super().__init__()
        patch_size         = if_none(config.model_vision_patch_size, [[4,2],[4,2],[2,2]])
        in_chans           = if_none(config.model_vision_in_chans, 3)
        embed_dim          = if_none(config.model_vision_embed_dim, [256, 384, 768])
        depth              = if_none(config.model_vision_depth, [2, 2, 11])
        num_heads          = if_none(config.model_vision_num_heads, 16)
        mlp_ratio          = if_none(config.model_vision_mlp_ratio, [4, 4, 4])
        qkv_bias           = if_none(config.model_vision_qkv_bias, False)
        qk_scale           = if_none(config.model_vision_qk_scale, None)
        drop_rate          = if_none(config.model_vision_drop_rate, 0.)
        attn_drop_rate     = if_none(config.model_vision_attn_drop_rate, 0.)
        drop_path_rate     = if_none(config.model_vision_drop_path_rate, 0.)
        norm_layer         = if_none(config.model_vision_norm_layer, nn.LayerNorm)
        # --------------------------------------------------------------------------
        # ConvMAE encoder specifics
        self.patch_embed1 = PatchEmbed(
                patch_size=patch_size[0], in_chans=in_chans, embed_dim=embed_dim[0])
        self.patch_embed2 = PatchEmbed(
                patch_size=patch_size[1], in_chans=embed_dim[0], embed_dim=embed_dim[1])
        self.patch_embed3 = PatchEmbed(
                patch_size=patch_size[2], in_chans=embed_dim[1], embed_dim=embed_dim[2])
        self.in_chans = in_chans
        self.patch_embed4 = nn.Linear(embed_dim[2], embed_dim[2])

        # Hard code for max num patches
        # Since we init weight using sqrt(num_patches), keep in mind that num patches is a square number
        self.num_patches = 1024 

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depth))]  # stochastic depth decay rule
        # Largest patchsize
        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, embed_dim[2]), requires_grad=False)
        self.pos_drop = nn.Dropout(p=drop_rate)
        self.blocks1 = nn.ModuleList([
            CBlock(
                dim=embed_dim[0],
                num_heads=num_heads,
                mlp_ratio=mlp_ratio[0],
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[depth[0] + depth[1] + i],
                norm_layer=norm_layer)
            for i in range(depth[0])])
        self.blocks2 = nn.ModuleList([
            CBlock(
                dim=embed_dim[1],
                num_heads=num_heads,
                mlp_ratio=mlp_ratio[1],
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[depth[0] + depth[1] + i],
                norm_layer=norm_layer)
            for i in range(depth[1])])
        self.blocks3 = nn.ModuleList([
            Block(
                dim=embed_dim[2],
                num_heads=num_heads,
                mlp_ratio=mlp_ratio[2],
                qkv_bias=qkv_bias,
                qk_scale=qk_scale,
                drop=drop_rate,
                attn_drop=attn_drop_rate,
                drop_path=dpr[depth[0] + depth[1] + i],
                norm_layer=norm_layer)
            for i in range(depth[2])])
        self.norm = norm_layer(embed_dim[-1])

        trunc_normal_(self.pos_embed, std=.02)
        self.apply(self._init_weights)
        if config.model_vision_mcmae_checkpoint is not None:
            logging.info(f'Read vision MCMAE from {config.model_vision_mcmae_checkpoint}.')
            state_dict = torch.load(config.model_vision_mcmae_checkpoint, map_location=None)

            for name, param in self.named_parameters():
                if name not in state_dict:
                    logging.info('{} not found'.format(name))
                elif state_dict[name].shape != param.shape:
                    logging.info('{} missmatching shape, required {} but found {}'.format(name, param.shape, state_dict[name].shape))
                    del state_dict[name]

            self.load_state_dict(state_dict, strict=False)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    def forward(self, x):
        # Stage 1
        x = self.patch_embed1(x)
        for blk in self.blocks1:
            x = blk(x)
        # Stage 2
        x = self.patch_embed2(x)
        for blk in self.blocks2:
            x = blk(x)
        # Stage 3
        x = self.patch_embed3(x)
        _, _, h, w = x.shape
        x = rearrange(x,'b c h w -> b (w h) c')
        x = self.patch_embed4(x)
        # Add pos embed w/o cls token
        x = x + self.pos_embed[:,:x.shape[1],:]
        # apply Transformer blocks
        for blk in self.blocks3:
            x = blk(x)
        x = self.norm(x)
        x = rearrange(x,'b (w h) c -> b c h w', h=h,w=w)
        return x