global:
  name: pretrain-language-model
  phase: train
  stage: pretrain-language
  workdir: workdir
  seed: ~

dataset:
  train: {
    roots: [
        'data/vi1.csv',
    ],
    batch_size: 16
  }
  valid: {
    roots: [
        'data/vi2.csv',
    ],
    batch_size: 4
  }
  charset_path: data/charset_227.txt
  case_sensitive: True
  space_as_token: False

training:
  epochs: 15
  show_iters: 50
  eval_iters: 6000
  save_iters: 3000

optimizer:
  type: Adam
  true_wd: False
  wd: 0.0
  bn_wd: False
  clip_grad: 20
  lr: 0.0001
  scheduler: {
    periods: [ 12, 3 ],
    gamma: 0.1,
  }

model:
  name: 'semimtr.modules.model_language.BCNLanguage'
  language: {
    d_model: 768,
    nhead: 4,
    d_inner: 1024,
    num_layers: 4,
    loss_weight: 1.,
    use_self_attn: False
  }
