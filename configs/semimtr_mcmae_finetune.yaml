global:
  name: consistency-regularization
  phase: train
  stage: train-semi-supervised
  workdir: workdir
  seed: ~

dataset:
  scheme: consistency_regularization
  type: ST
  resize_type: 'padded'
  train: {
    roots: [
        'data/training/label',
        'data/training/unlabel',
    ],
    batch_size: 2,
    weights: ~ #[ 0.5, 0.5 ]
  }
  valid: {
    roots: [
        'data/validation',
    ],
    batch_size: 2
  }
  test: {
    roots: [
        'data/evaluation',
    ],
    batch_size: 2
  }
  image: {
    height: 32,
    width: 512,
  }
  data_aug: True
  multiscales: False
  num_workers: 0
  augmentation_severity: 1
  charset_path: 'data/charset_227.txt'

training:
  epochs: 1
  show_iters: 50
  eval_iters: 1000
  save_iters: 3000

optimizer:
  type: Adam
  true_wd: False
  wd: 0.0001
  bn_wd: False
  clip_grad: 20
  lr: 0.0001
  scheduler: {
    periods: [ 3, 1, 1 ],
    gamma: 0.1,
  }

model:
  name: 'semimtr.modules.model_fusion_consistency_regularization.ConsistencyRegularizationFusionModel'
  iter_size: 1
  vision: {
    # checkpoint: workdir/semimtr_vision_model_real_l_and_u.pth,
    loss_weight: 1.,
    attention: 'position',
    backbone: 'mcmae',
    backbone_ln: 3,
    # checkpoint_submodule: vision,
  }
  language: {
    # checkpoint: workdir/abinet_language_model.pth,
    d_model: 768,
    num_layers: 4,
    loss_weight: 1.,
    detach: True,
    use_self_attn: False
  }
  alignment: {
    d_model: 768,
    loss_weight: 1.,
  }
  consistency: {
    loss_weight: 1.,
    supervised_flag: True,
    all_to_all: True,
    #    teacher_layer: vision,  # alignment | language | vision (doesn't matter if all_to_all is True)
    #    student_layer: all,  # all | alignment | language | vision (doesn't matter if all_to_all is True)
    teacher_one_hot: True,
    kl_div: False,
    teacher_stop_gradients: True,
    use_threshold: False,
    ema: False,
    ema_decay: 0.9999
  }
